{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Logistic regression for multi-class classification\n",
        "\n",
        "### * A basic implementation, on MNIST digits dataset, that includes a single linear layer (from 28x28 greyscale image to a length-10 activation vector), followed by a soft-max operation, minimizing the standard cross-entropy loss.\n",
        "\n",
        "### * Training is done under 4 (2x2) different settings of the hyper-parameters.\n",
        "\n",
        "### * There are missing pieces of code that you should fill in (notice the  - <font color='red'>EDIT CODE</font> messages).\n",
        "\n",
        "### * You are required to submit this ipynb file, <font color='blue'>including the executed output blocks</font>.\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "K29I-OwCEYzW",
        "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] import libraries"
      ],
      "metadata": {
        "id": "R0GhOP4bi5q7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "metadata": {
        "id": "yg0hGYO7i5q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2] load data"
      ],
      "metadata": {
        "id": "-Cq_rZ20i5q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename_data   = './data/assignment_05_data.npz'\n",
        "if os.path.exists(filename_data):\n",
        "  print('data already exists')\n",
        "else:\n",
        "  print('downloading data...')\n",
        "  !mkdir './data'\n",
        "  !wget -O './data/assignment_05_data.npz' https://www.cs.haifa.ac.il/~skorman/assignment_05_data.npz"
      ],
      "metadata": {
        "id": "uRWwWRBuoG8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135f4dbc-c13c-408e-9262-79aad9d5251a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading data...\n",
            "--2023-06-18 12:27:26--  https://www.cs.haifa.ac.il/~skorman/assignment_05_data.npz\n",
            "Resolving www.cs.haifa.ac.il (www.cs.haifa.ac.il)... 132.74.123.100\n",
            "Connecting to www.cs.haifa.ac.il (www.cs.haifa.ac.il)|132.74.123.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90049002 (86M)\n",
            "Saving to: ‘./data/assignment_05_data.npz’\n",
            "\n",
            "./data/assignment_0 100%[===================>]  85.88M  9.60MB/s    in 11s     \n",
            "\n",
            "2023-06-18 12:27:40 (7.68 MB/s) - ‘./data/assignment_05_data.npz’ saved [90049002/90049002]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "directory_data  = './data/'\n",
        "filename_data   = 'assignment_05_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "x_train = data['x_train']\n",
        "y_train = data['y_train']\n",
        "\n",
        "x_test  = data['x_test']\n",
        "y_test  = data['y_test']\n",
        "\n",
        "num_data_train  = x_train.shape[0]\n",
        "num_data_test   = x_test.shape[0]\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of x_train :', x_train.shape)\n",
        "print('size of y_train :', y_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of x_test :', x_test.shape)\n",
        "print('size of y_test :', y_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training images :', x_train.shape[0])\n",
        "print('height of training images :', x_train.shape[1])\n",
        "print('width of training images :', x_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing images :', x_test.shape[0])\n",
        "print('height of testing images :', x_test.shape[1])\n",
        "print('width of testing images :', x_test.shape[2])\n",
        "print('*************************************************')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************************************\n",
            "size of x_train : (20000, 28, 28)\n",
            "size of y_train : (20000, 10)\n",
            "*************************************************\n",
            "size of x_test : (8000, 28, 28)\n",
            "size of y_test : (8000, 10)\n",
            "*************************************************\n",
            "number of training images : 20000\n",
            "height of training images : 28\n",
            "width of training images : 28\n",
            "*************************************************\n",
            "number of testing images : 8000\n",
            "height of testing images : 28\n",
            "width of testing images : 28\n",
            "*************************************************\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxyPUZ3Ri5q8",
        "outputId": "7cdce22e-210d-4866-e41e-448f3c6bd852"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3] number of classes"
      ],
      "metadata": {
        "id": "M-bjnGHoi5q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nClass = y_train.shape[1]\n",
        "\n",
        "print('*************************************************')\n",
        "print('number of classes :', nClass)\n",
        "print('*************************************************')"
      ],
      "outputs": [],
      "metadata": {
        "id": "5RGhAiEpi5q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4] vectorize image data"
      ],
      "metadata": {
        "id": "gewSt839i5q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "vector_x_train  = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "vector_x_test   = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "print('*************************************************')\n",
        "print('dimension of the training data :', vector_x_train.shape)\n",
        "print('dimension of the testing data :', vector_x_test.shape)\n",
        "print('*************************************************')\n",
        "print('dimension of the training label :', y_train.shape)\n",
        "print('dimension of the testing label :', y_test.shape)\n",
        "print('*************************************************')"
      ],
      "outputs": [],
      "metadata": {
        "id": "3vpU8_Pfi5q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5] index for each class"
      ],
      "metadata": {
        "id": "_Q1qJ4tki5q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "index_train = {}\n",
        "index_test  = {}\n",
        "\n",
        "number_index_train  = np.zeros(nClass)\n",
        "number_index_test   = np.zeros(nClass)\n",
        "\n",
        "print('*************************************************')\n",
        "\n",
        "for i in range(nClass):\n",
        "\n",
        "    index_train[i]  = np.where(y_train[:, i] == 1)\n",
        "    index_test[i]   = np.where(y_test[:, i] == 1)\n",
        "\n",
        "    number_index_train[i]   = np.shape(index_train[i])[1]\n",
        "    number_index_test[i]    = np.shape(index_test[i])[1]\n",
        "\n",
        "    print('number of the training data for class %2d : %5d' % (i, number_index_train[i]))\n",
        "    print('number of the testing data for class %2d : %5d' % (i, number_index_test[i]))\n",
        "\n",
        "print('*************************************************')"
      ],
      "outputs": [],
      "metadata": {
        "id": "1oxsACH-i5q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6] plot data"
      ],
      "metadata": {
        "id": "zrRVmF-Mi5q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 1, nRow * 1))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "D7yTKYnCi5q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nRow    = 2\n",
        "nCol    = 4\n",
        "nPlot   = nRow * nCol"
      ],
      "outputs": [],
      "metadata": {
        "id": "OMnorMlRi5q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in range(nClass):\n",
        "    index_class_plot = index_train[i][0][0:nPlot]\n",
        "    plot_data_grid(x_train, index_class_plot, nRow, nCol)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2qQQTXl1i5q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7] linear layer"
      ],
      "metadata": {
        "id": "eet8fb0Zi5q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def layer_linear(input, weight):\n",
        "\n",
        "    output = np.matmul(input, np.transpose(weight))\n",
        "\n",
        "    return output"
      ],
      "outputs": [],
      "metadata": {
        "id": "QEzr0LHKrth9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8] softmax function - <font color='red'>EDIT CODE</font>\n"
      ],
      "metadata": {
        "id": "eReLs-ZUi5q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def activation_softmax(input):\n",
        "\n",
        "    output =\n",
        "\n",
        "    return output"
      ],
      "outputs": [],
      "metadata": {
        "id": "9TyUM553i5q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9] compute prediction by the forward propagation of the neural network - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "VMJfUOaci5q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_prediction(input, weight):\n",
        "\n",
        "    prediction =\n",
        "\n",
        "    return prediction"
      ],
      "outputs": [],
      "metadata": {
        "id": "xNMZmIqTi5rA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10] compute cross-entropy loss - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "DKmkbj1Ei5rA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_loss_cross_entropy(prediction, label):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "kTSET7b8i5rA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11] compute weight decay regularization term of loss - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "i5kQ1gVDi5rA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_loss_regularization(weight):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "HqAROZyzi5rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12] compute final loss function - <font color='red'>EDIT CODE</font>\n",
        "### - using the hyper-parameter lmbda to balance the cross-entropy and weight-decay (L=CE+lmbda*WD)"
      ],
      "metadata": {
        "id": "LovycKdTi5rB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_loss(prediction, label, lmbda, weight):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "kG2LGd_Oi5rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13] compute gradient for the cross-entropy term  - <font color='red'>EDIT CODE</font>\n",
        "(follow, for example, https://jmlb.github.io/ml/2017/12/26/Calculate_Gradient_Softmax/)"
      ],
      "metadata": {
        "id": "oZFNcxizi5rB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_gradient_cross_entropy(input, prediction, label):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ],
      "outputs": [],
      "metadata": {
        "id": "WcseJGoHi5rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14] compute gradient for the regularization term - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "Wga7r5XJi5rC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_gradient_regularization(lmbda, weight):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ],
      "outputs": [],
      "metadata": {
        "id": "x-51-m4di5rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15] compute final combined gradient - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "ZfyGkC3Li5rD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_gradient(input, prediction, label, lmbda, weight):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ],
      "outputs": [],
      "metadata": {
        "id": "snDErvAzi5rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16] compute accuracy - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "wncF7QgRi5rD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_accuracy(prediction, label):\n",
        "\n",
        "    accuracy =\n",
        "\n",
        "    return accuracy"
      ],
      "outputs": [],
      "metadata": {
        "id": "l2-pdC4Wi5rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17] consider bias in the data\n",
        "\n",
        "### - bias represented by extending the input with a '1' scalar\n"
      ],
      "metadata": {
        "id": "wPUEceGMi5rD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "one_train   = np.ones((x_train.shape[0], 1))\n",
        "one_test    = np.ones((x_test.shape[0], 1))\n",
        "\n",
        "vector_x_train_bias = np.concatenate((vector_x_train, one_train), axis=1)\n",
        "vector_x_test_bias  = np.concatenate((vector_x_test, one_test), axis=1)\n",
        "\n",
        "print('dimension of the training data with bias :', vector_x_train_bias.shape)\n",
        "print('dimension of the testing data with bias :', vector_x_test_bias.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "STjZeZhci5rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18] construct model parameters and initialize them - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "op9Cxsimi5rE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_weight(dim_input, dim_output):\n",
        "\n",
        "    weight =\n",
        "    # initialize the model parameters (linear = 0.001, bias = 1)\n",
        "    weight[...] = 0.001\n",
        "    weight[...] = 1\n",
        "\n",
        "    return weight"
      ],
      "outputs": [],
      "metadata": {
        "id": "ajbfA0KKi5rE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dim_input   =\n",
        "dim_output  =\n",
        "\n",
        "weight = get_weight(dim_input, dim_output)\n",
        "\n",
        "print('dimension of the model parameters: ', weight.shape)\n",
        "print('first row of the weight matrix: ', weight[0, :])"
      ],
      "outputs": [],
      "metadata": {
        "id": "coNT06YMi5rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19] hyper-parameters"
      ],
      "metadata": {
        "id": "Phyf-bmyi5rI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "number_epochs    = 800\n",
        "learning_rate       = 0.001\n",
        "\n",
        "list_size_minibatch = [50, 100]\n",
        "list_weight_decay   = [0.001, 0.01]\n",
        "\n",
        "num_size_minibatch  = len(list_size_minibatch)\n",
        "num_weight_decay    = len(list_weight_decay)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y-XGV2Kzi5rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20] variables for optimization information (for different minibatch sizes)"
      ],
      "metadata": {
        "id": "VFHYCQxfi5rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_loss_mean_minibatch     = np.zeros((num_size_minibatch, number_epochs))\n",
        "train_loss_std_minibatch      = np.zeros((num_size_minibatch, number_epochs))\n",
        "\n",
        "train_accuracy_mean_minibatch = np.zeros((num_size_minibatch, number_epochs))\n",
        "train_accuracy_std_minibatch  = np.zeros((num_size_minibatch, number_epochs))\n",
        "\n",
        "test_loss_minibatch           = np.zeros((num_size_minibatch, number_epochs))\n",
        "test_accuracy_minibatch       = np.zeros((num_size_minibatch, number_epochs))"
      ],
      "outputs": [],
      "metadata": {
        "id": "TiWWC5U4i5rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21] variables for optimization information (for different weight decay values)"
      ],
      "metadata": {
        "id": "vHVSCo1Pi5rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_loss_mean_weight_decay        = np.zeros((num_weight_decay, number_epochs))\n",
        "train_loss_std_weight_decay         = np.zeros((num_weight_decay, number_epochs))\n",
        "\n",
        "train_accuracy_mean_weight_decay    = np.zeros((num_weight_decay, number_epochs))\n",
        "train_accuracy_std_weight_decay     = np.zeros((num_weight_decay, number_epochs))\n",
        "\n",
        "test_loss_weight_decay              = np.zeros((num_weight_decay, number_epochs))\n",
        "test_accuracy_weight_decay          = np.zeros((num_weight_decay, number_epochs))"
      ],
      "outputs": [],
      "metadata": {
        "id": "WWtYeIayi5rK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Training\n",
        "---"
      ],
      "metadata": {
        "id": "NfxlLc2APM4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1] SGD iterations with different mini-batch sizes (with weight decay = 0) - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "RULdRAJMi5rK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# iteration for mini-batch\n",
        "for k in range(num_size_minibatch):\n",
        "\n",
        "    size_minibatch  = list_size_minibatch[k]\n",
        "    num_minibatch   =\n",
        "    lmbda           = 0\n",
        "\n",
        "    print('mini-batch size = %3d, lmbda = %4.3f' % (size_minibatch, lmbda))\n",
        "\n",
        "    weight =\n",
        "\n",
        "    # initialze seed for generating random number\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # iteration for epoch\n",
        "    for i in tqdm(range(number_epochs)):\n",
        "\n",
        "        index_shuffle   = np.random.permutation(num_data_train)\n",
        "        loss_epoch      = []\n",
        "        accuracy_epoch  = []\n",
        "\n",
        "        for j in range(num_minibatch):\n",
        "\n",
        "            index_minibatch = index_shuffle[j * size_minibatch : (j+1) * size_minibatch]\n",
        "\n",
        "            data    =\n",
        "            label   =\n",
        "\n",
        "            prediction  =\n",
        "            gradient    =\n",
        "            # update network weights:\n",
        "            weight      =\n",
        "\n",
        "            # compute measures after update:\n",
        "            prediction  =\n",
        "            loss        =\n",
        "            accuracy    =\n",
        "\n",
        "            loss_epoch.append(loss)\n",
        "            accuracy_epoch.append(accuracy)\n",
        "\n",
        "        train_loss_mean_minibatch[k, i] =\n",
        "        train_loss_std_minibatch[k, i]  =\n",
        "\n",
        "        train_accuracy_mean_minibatch[k, i] =\n",
        "        train_accuracy_std_minibatch[k, i]  =\n",
        "\n",
        "        # testing\n",
        "        data    =\n",
        "        label   =\n",
        "\n",
        "        prediction      =\n",
        "        loss_test       =\n",
        "        accuracy_test   =\n",
        "\n",
        "        test_loss_minibatch[k, i]       =\n",
        "        test_accuracy_minibatch[k, i]   ="
      ],
      "outputs": [],
      "metadata": {
        "id": "wPeeaEJ_i5rK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2] SGD iterations with different weight decay parameter (with mini-batch size = 100) - <font color='red'>EDIT CODE</font>"
      ],
      "metadata": {
        "id": "VkVau04wi5rL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# iteration for mini-batch\n",
        "for k in range(num_weight_decay):\n",
        "\n",
        "    size_minibatch  = 100\n",
        "    num_minibatch   =\n",
        "    lmbda           = list_weight_decay[k]\n",
        "\n",
        "    print('mini-batch size = %3d, lmbda = %4.3f' % (size_minibatch, lmbda))\n",
        "\n",
        "    weight =\n",
        "    # initialze seed for generating random number\n",
        "    np.random.seed(0)\n",
        "    # iteration for epoch\n",
        "    for i in tqdm(range(number_epochs)):\n",
        "\n",
        "        index_shuffle   = np.random.permutation(num_data_train)\n",
        "        loss_epoch      = []\n",
        "        accuracy_epoch  = []\n",
        "\n",
        "        for j in range(num_minibatch):\n",
        "\n",
        "            index_minibatch = index_shuffle[j * size_minibatch : (j+1) * size_minibatch]\n",
        "\n",
        "            data    =\n",
        "            label   =\n",
        "\n",
        "            prediction  =\n",
        "            gradient    =\n",
        "            # update network weights:\n",
        "            weight      =\n",
        "\n",
        "            # compute measures after update:\n",
        "            prediction  =\n",
        "            loss        =\n",
        "            accuracy    =\n",
        "\n",
        "            loss_epoch.append(loss)\n",
        "            accuracy_epoch.append(accuracy)\n",
        "\n",
        "        train_loss_mean_weight_decay[k, i] =\n",
        "        train_loss_std_weight_decay[k, i]  =\n",
        "\n",
        "        train_accuracy_mean_weight_decay[k, i] =\n",
        "        train_accuracy_std_weight_decay[k, i]  =\n",
        "\n",
        "        # testing\n",
        "        data    =\n",
        "        label   =\n",
        "\n",
        "        prediction      =\n",
        "        loss_test       =\n",
        "        accuracy_test   =\n",
        "\n",
        "        test_loss_weight_decay[k, i]       =\n",
        "        test_accuracy_weight_decay[k, i]   =\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nIA4Xtghi5rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# RESULTS\n",
        "---"
      ],
      "metadata": {
        "id": "eQvKviH3rtiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] plot curve (helper functions)"
      ],
      "metadata": {
        "id": "AJyAHW8brtiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_curve(data, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.plot(range(len(data)), data, '-', color='red')\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "95yUiMc4rtiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.title(title)\n",
        "\n",
        "    lmbda = 0.3\n",
        "\n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', lmbda = lmbda)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ie2h2ezurtiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_curve2(data1, label_data1, data2, label_data2, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.plot(range(len(data1)), data1, '-', color = 'blue', label = label_data1)\n",
        "    plt.plot(range(len(data2)), data2, '-', color = 'red', label = label_data2)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "M2QXqSTArtiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_curve_error2(data1_mean, data1_std, data1_label, data2_mean, data2_std, data2_label, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(title)\n",
        "\n",
        "    lmbda = 0.3\n",
        "\n",
        "    plt.plot(range(len(data1_mean)), data1_mean, '-', color = 'blue', label = data1_label)\n",
        "    plt.fill_between(range(len(data1_mean)), data1_mean - data1_std, data1_mean + data1_std, facecolor = 'blue', lmbda = lmbda)\n",
        "\n",
        "    plt.plot(range(len(data2_mean)), data2_mean, '-', color = 'red', label = data2_label)\n",
        "    plt.fill_between(range(len(data2_mean)), data2_mean - data2_std, data2_mean + data2_std, facecolor = 'red', lmbda = lmbda)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Yw_o9g9srtiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2] Display loss and accuracy curves (over train and test sets)"
      ],
      "metadata": {
        "id": "NrV5x4Odi5rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curve_error2(train_loss_mean_minibatch[0], train_loss_std_minibatch[0], 'mini-batch size = 50',\n",
        "                  train_loss_mean_minibatch[1], train_loss_std_minibatch[1], 'mini-batch size = 100',\n",
        "                  'epoch', 'loss', 'loss (training): for diff. mini-batch sizes')\n",
        "plot_curve_error2(train_accuracy_mean_minibatch[0], train_accuracy_std_minibatch[0], 'mini-batch size = 50',\n",
        "                  train_accuracy_mean_minibatch[1], train_accuracy_std_minibatch[1], 'mini-batch size = 100',\n",
        "                  'epoch', 'accuracy', 'accuracy (training): for diff. mini-batch sizes')\n",
        "plot_curve_error2(train_loss_mean_weight_decay[0], train_loss_std_weight_decay[0], 'weight-decay = 0.001',\n",
        "                  train_loss_mean_weight_decay[1], train_loss_std_weight_decay[1], 'weight-decay = 0.01',\n",
        "                  'epoch', 'loss', 'loss (training): for diff. weight-decay values')\n",
        "plot_curve_error2(train_accuracy_mean_weight_decay[0], train_accuracy_std_weight_decay[0], 'weight-decay = 0.001',\n",
        "                  train_accuracy_mean_weight_decay[1], train_accuracy_std_weight_decay[1], 'weight-decay = 0.01',\n",
        "                  'epoch', 'accuracy', 'accuracy (training): for diff. weight-decay values')"
      ],
      "metadata": {
        "id": "mBwd30BXM1qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot_curve2(test_loss_minibatch[0], 'mini-batch = 50', test_loss_minibatch[1], 'mini-batch = 100', 'epoch', 'loss', 'loss (testing) for diff. mini-batch sizes')\n",
        "plot_curve2(test_accuracy_minibatch[0], 'mini-batch = 50', test_accuracy_minibatch[1], 'mini-batch = 100', 'epoch', 'accuracy', 'accuracy (testing) for diff. mini-batch sizes')\n",
        "plot_curve2(test_loss_weight_decay[0], 'weight-decay = 0.001', test_loss_weight_decay[1], 'weight-decay = 0.01', 'epoch', 'loss', 'loss (testing) for diff weight-decay values')\n",
        "plot_curve2(test_accuracy_weight_decay[0], 'weight-decay = 0.001', test_accuracy_weight_decay[1], 'weight-decay = 0.01', 'epoch', 'accuracy', 'accuracy (testing) for diff weight-decay values')"
      ],
      "outputs": [],
      "metadata": {
        "id": "KM9i5mkCi5rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3] Show mispredictions - <font color='red'>EDIT CODE</font>\n",
        "### - Visualize 5 random misclassified train images and 5 random misclassified test images.\n",
        "### - For each, display the image, the predicted distribution, the true and predicted label. Make sure to visualize in a compact and clear manner."
      ],
      "metadata": {
        "id": "JcOmkO6IQYDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... complete here ..."
      ],
      "metadata": {
        "id": "AQMSr5g1T096"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4] Present final results  - <font color='red'>EDIT CODE</font>\n",
        "### - Display a table with the final results - test accuracies for each of the tested configurations."
      ],
      "metadata": {
        "id": "HoxoMb9jUECc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... complete here ..."
      ],
      "metadata": {
        "id": "Va2Q8wd8UcKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5] Improve results - <font color='red'>EDIT CODE</font>\n",
        "## * Implement two different simple extensions / modifications / configurations that improve the best accuracy achieved above by at least 1%. Do not use a larger number of training epochs.\n",
        "## * Do not make any special effort to get the best result possible, but simply reach this goal.\n",
        "### - Insert the required code blocks here below.\n",
        "### - At the very end (see below), present the final result (test accuracy)"
      ],
      "metadata": {
        "id": "ja0nUf4pUnYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code block 1"
      ],
      "metadata": {
        "id": "JMsnfsTFV3a6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQKqSTB_VzcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code block 2"
      ],
      "metadata": {
        "id": "3lfJySJ-V7uL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syXQHswUUmfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code block ..."
      ],
      "metadata": {
        "id": "5IeVIpHDWAke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final result - <font color='red'>EDIT TEXT and CODE</font>\n",
        "### - Edit this text here to explain very shortly what you have done.\n",
        "### - Use the code block below to present your final results (test accuracy only)."
      ],
      "metadata": {
        "id": "qptS4SZ3WELu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtvTMi5AXlIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}