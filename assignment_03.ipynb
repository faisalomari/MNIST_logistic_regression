{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K29I-OwCEYzW",
        "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
      },
      "source": [
        "---\n",
        "---\n",
        "# Logistic regression for multi-class classification\n",
        "\n",
        "### * A basic implementation, on MNIST digits dataset, that includes a single linear layer (from 28x28 greyscale image to a length-10 activation vector), followed by a soft-max operation, minimizing the standard cross-entropy loss.\n",
        "\n",
        "### * Training is done under 4 (2x2) different settings of the hyper-parameters.\n",
        "\n",
        "### * There are missing pieces of code that you should fill in (notice the  - <font color='red'>EDIT CODE</font> messages).\n",
        "\n",
        "### * You are required to submit this ipynb file, <font color='blue'>including the executed output blocks</font>.\n",
        "---\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R0GhOP4bi5q7"
      },
      "source": [
        "## 1] import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yg0hGYO7i5q8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cq_rZ20i5q8"
      },
      "source": [
        "## 2] load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRWwWRBuoG8K",
        "outputId": "135f4dbc-c13c-408e-9262-79aad9d5251a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading data...\n",
            "data downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "filename_data = './data/assignment_05_data.npz'\n",
        "\n",
        "if os.path.exists(filename_data):\n",
        "    print('data already exists')\n",
        "else:\n",
        "    print('downloading data...')\n",
        "    os.makedirs('./data', exist_ok=True)\n",
        "    url = 'https://www.cs.haifa.ac.il/~skorman/assignment_05_data.npz'\n",
        "    urllib.request.urlretrieve(url, filename_data)\n",
        "    print('data downloaded successfully')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxyPUZ3Ri5q8",
        "outputId": "7cdce22e-210d-4866-e41e-448f3c6bd852"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/assignment_05_data.npz'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m directory_data  \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m filename_data   \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39massignment_05_data.npz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m data            \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(directory_data, filename_data))\n\u001b[0;32m      5\u001b[0m x_train \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mx_train\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m y_train \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39my_train\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\omari\\.conda\\envs\\diarization\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/assignment_05_data.npz'"
          ]
        }
      ],
      "source": [
        "directory_data  = './data/'\n",
        "filename_data   = 'assignment_05_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "x_train = data['x_train']\n",
        "y_train = data['y_train']\n",
        "\n",
        "x_test  = data['x_test']\n",
        "y_test  = data['y_test']\n",
        "\n",
        "num_data_train  = x_train.shape[0]\n",
        "num_data_test   = x_test.shape[0]\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of x_train :', x_train.shape)\n",
        "print('size of y_train :', y_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of x_test :', x_test.shape)\n",
        "print('size of y_test :', y_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training images :', x_train.shape[0])\n",
        "print('height of training images :', x_train.shape[1])\n",
        "print('width of training images :', x_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing images :', x_test.shape[0])\n",
        "print('height of testing images :', x_test.shape[1])\n",
        "print('width of testing images :', x_test.shape[2])\n",
        "print('*************************************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bjnGHoi5q9"
      },
      "source": [
        "## 3] number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RGhAiEpi5q9"
      },
      "outputs": [],
      "source": [
        "nClass = y_train.shape[1]\n",
        "\n",
        "print('*************************************************')\n",
        "print('number of classes :', nClass)\n",
        "print('*************************************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gewSt839i5q9"
      },
      "source": [
        "## 4] vectorize image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vpU8_Pfi5q-"
      },
      "outputs": [],
      "source": [
        "vector_x_train  = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "vector_x_test   = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "print('*************************************************')\n",
        "print('dimension of the training data :', vector_x_train.shape)\n",
        "print('dimension of the testing data :', vector_x_test.shape)\n",
        "print('*************************************************')\n",
        "print('dimension of the training label :', y_train.shape)\n",
        "print('dimension of the testing label :', y_test.shape)\n",
        "print('*************************************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q1qJ4tki5q-"
      },
      "source": [
        "## 5] index for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oxsACH-i5q-"
      },
      "outputs": [],
      "source": [
        "index_train = {}\n",
        "index_test  = {}\n",
        "\n",
        "number_index_train  = np.zeros(nClass)\n",
        "number_index_test   = np.zeros(nClass)\n",
        "\n",
        "print('*************************************************')\n",
        "\n",
        "for i in range(nClass):\n",
        "\n",
        "    index_train[i]  = np.where(y_train[:, i] == 1)\n",
        "    index_test[i]   = np.where(y_test[:, i] == 1)\n",
        "\n",
        "    number_index_train[i]   = np.shape(index_train[i])[1]\n",
        "    number_index_test[i]    = np.shape(index_test[i])[1]\n",
        "\n",
        "    print('number of the training data for class %2d : %5d' % (i, number_index_train[i]))\n",
        "    print('number of the testing data for class %2d : %5d' % (i, number_index_test[i]))\n",
        "\n",
        "print('*************************************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zrRVmF-Mi5q-"
      },
      "source": [
        "## 6] plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7yTKYnCi5q-"
      },
      "outputs": [],
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 1, nRow * 1))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMnorMlRi5q_"
      },
      "outputs": [],
      "source": [
        "nRow    = 2\n",
        "nCol    = 4\n",
        "nPlot   = nRow * nCol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qQQTXl1i5q_"
      },
      "outputs": [],
      "source": [
        "for i in range(nClass):\n",
        "    index_class_plot = index_train[i][0][0:nPlot]\n",
        "    plot_data_grid(x_train, index_class_plot, nRow, nCol)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eet8fb0Zi5q_"
      },
      "source": [
        "## 7] linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEzr0LHKrth9"
      },
      "outputs": [],
      "source": [
        "def layer_linear(input, weight):\n",
        "\n",
        "    output = \n",
        "\n",
        "    return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eReLs-ZUi5q_"
      },
      "source": [
        "## 8] softmax function - <font color='red'>EDIT CODE</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TyUM553i5q_"
      },
      "outputs": [],
      "source": [
        "def activation_softmax(input):\n",
        "\n",
        "    output = np.exp(input) / np.sum(np.exp(input), axis=1, keepdims=True)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VMJfUOaci5q_"
      },
      "source": [
        "## 9] compute prediction by the forward propagation of the neural network - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNMZmIqTi5rA"
      },
      "outputs": [],
      "source": [
        "def compute_prediction(input, weight):\n",
        "\n",
        "    prediction =\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DKmkbj1Ei5rA"
      },
      "source": [
        "## 10] compute cross-entropy loss - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTSET7b8i5rA"
      },
      "outputs": [],
      "source": [
        "def compute_loss_cross_entropy(prediction, label):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i5kQ1gVDi5rA"
      },
      "source": [
        "## 11] compute weight decay regularization term of loss - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqAROZyzi5rB"
      },
      "outputs": [],
      "source": [
        "def compute_loss_regularization(weight):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LovycKdTi5rB"
      },
      "source": [
        "## 12] compute final loss function - <font color='red'>EDIT CODE</font>\n",
        "### - using the hyper-parameter lmbda to balance the cross-entropy and weight-decay (L=CE+lmbda*WD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG2LGd_Oi5rB"
      },
      "outputs": [],
      "source": [
        "def compute_loss(prediction, label, lmbda, weight):\n",
        "\n",
        "    loss =\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFNcxizi5rB"
      },
      "source": [
        "## 13] compute gradient for the cross-entropy term  - <font color='red'>EDIT CODE</font>\n",
        "(follow, for example, https://jmlb.github.io/ml/2017/12/26/Calculate_Gradient_Softmax/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcseJGoHi5rC"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_cross_entropy(input, prediction, label):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wga7r5XJi5rC"
      },
      "source": [
        "## 14] compute gradient for the regularization term - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-51-m4di5rC"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_regularization(lmbda, weight):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfyGkC3Li5rD"
      },
      "source": [
        "## 15] compute final combined gradient - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snDErvAzi5rD"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(input, prediction, label, lmbda, weight):\n",
        "\n",
        "    gradient =\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wncF7QgRi5rD"
      },
      "source": [
        "## 16] compute accuracy - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2-pdC4Wi5rD"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(prediction, label):\n",
        "\n",
        "    accuracy =\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUEceGMi5rD"
      },
      "source": [
        "## 17] consider bias in the data\n",
        "\n",
        "### - bias represented by extending the input with a '1' scalar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjZeZhci5rE"
      },
      "outputs": [],
      "source": [
        "one_train   = np.ones((x_train.shape[0], 1))\n",
        "one_test    = np.ones((x_test.shape[0], 1))\n",
        "\n",
        "vector_x_train_bias = np.concatenate((vector_x_train, one_train), axis=1)\n",
        "vector_x_test_bias  = np.concatenate((vector_x_test, one_test), axis=1)\n",
        "\n",
        "print('dimension of the training data with bias :', vector_x_train_bias.shape)\n",
        "print('dimension of the testing data with bias :', vector_x_test_bias.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "op9Cxsimi5rE"
      },
      "source": [
        "## 18] construct model parameters and initialize them - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajbfA0KKi5rE"
      },
      "outputs": [],
      "source": [
        "def get_weight(dim_input, dim_output):\n",
        "\n",
        "    weight =\n",
        "    # initialize the model parameters (linear = 0.001, bias = 1)\n",
        "    weight[...] = 0.001\n",
        "    weight[...] = 1\n",
        "\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coNT06YMi5rI"
      },
      "outputs": [],
      "source": [
        "dim_input   =\n",
        "dim_output  =\n",
        "\n",
        "weight = get_weight(dim_input, dim_output)\n",
        "\n",
        "print('dimension of the model parameters: ', weight.shape)\n",
        "print('first row of the weight matrix: ', weight[0, :])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Phyf-bmyi5rI"
      },
      "source": [
        "## 19] hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-XGV2Kzi5rJ"
      },
      "outputs": [],
      "source": [
        "number_epochs    = 800\n",
        "learning_rate       = 0.001\n",
        "\n",
        "list_size_minibatch = [50, 100]\n",
        "list_weight_decay   = [0.001, 0.01]\n",
        "\n",
        "num_size_minibatch  = len(list_size_minibatch)\n",
        "num_weight_decay    = len(list_weight_decay)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VFHYCQxfi5rJ"
      },
      "source": [
        "## 20] variables for optimization information (for different minibatch sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiWWC5U4i5rJ"
      },
      "outputs": [],
      "source": [
        "train_loss_mean_minibatch     = np.zeros((num_size_minibatch, number_epochs))\n",
        "train_loss_std_minibatch      = np.zeros((num_size_minibatch, number_epochs))\n",
        "\n",
        "train_accuracy_mean_minibatch = np.zeros((num_size_minibatch, number_epochs))\n",
        "train_accuracy_std_minibatch  = np.zeros((num_size_minibatch, number_epochs))\n",
        "\n",
        "test_loss_minibatch           = np.zeros((num_size_minibatch, number_epochs))\n",
        "test_accuracy_minibatch       = np.zeros((num_size_minibatch, number_epochs))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vHVSCo1Pi5rJ"
      },
      "source": [
        "## 21] variables for optimization information (for different weight decay values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWtYeIayi5rK"
      },
      "outputs": [],
      "source": [
        "train_loss_mean_weight_decay        = np.zeros((num_weight_decay, number_epochs))\n",
        "train_loss_std_weight_decay         = np.zeros((num_weight_decay, number_epochs))\n",
        "\n",
        "train_accuracy_mean_weight_decay    = np.zeros((num_weight_decay, number_epochs))\n",
        "train_accuracy_std_weight_decay     = np.zeros((num_weight_decay, number_epochs))\n",
        "\n",
        "test_loss_weight_decay              = np.zeros((num_weight_decay, number_epochs))\n",
        "test_accuracy_weight_decay          = np.zeros((num_weight_decay, number_epochs))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxlLc2APM4h"
      },
      "source": [
        "---\n",
        "# Training\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RULdRAJMi5rK"
      },
      "source": [
        "### 1] SGD iterations with different mini-batch sizes (with weight decay = 0) - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPeeaEJ_i5rK"
      },
      "outputs": [],
      "source": [
        "# iteration for mini-batch\n",
        "for k in range(num_size_minibatch):\n",
        "\n",
        "    size_minibatch  = list_size_minibatch[k]\n",
        "    num_minibatch   =\n",
        "    lmbda           = 0\n",
        "\n",
        "    print('mini-batch size = %3d, lmbda = %4.3f' % (size_minibatch, lmbda))\n",
        "\n",
        "    weight =\n",
        "\n",
        "    # initialze seed for generating random number\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # iteration for epoch\n",
        "    for i in tqdm(range(number_epochs)):\n",
        "\n",
        "        index_shuffle   = np.random.permutation(num_data_train)\n",
        "        loss_epoch      = []\n",
        "        accuracy_epoch  = []\n",
        "\n",
        "        for j in range(num_minibatch):\n",
        "\n",
        "            index_minibatch = index_shuffle[j * size_minibatch : (j+1) * size_minibatch]\n",
        "\n",
        "            data    =\n",
        "            label   =\n",
        "\n",
        "            prediction  =\n",
        "            gradient    =\n",
        "            # update network weights:\n",
        "            weight      =\n",
        "\n",
        "            # compute measures after update:\n",
        "            prediction  =\n",
        "            loss        =\n",
        "            accuracy    =\n",
        "\n",
        "            loss_epoch.append(loss)\n",
        "            accuracy_epoch.append(accuracy)\n",
        "\n",
        "        train_loss_mean_minibatch[k, i] =\n",
        "        train_loss_std_minibatch[k, i]  =\n",
        "\n",
        "        train_accuracy_mean_minibatch[k, i] =\n",
        "        train_accuracy_std_minibatch[k, i]  =\n",
        "\n",
        "        # testing\n",
        "        data    =\n",
        "        label   =\n",
        "\n",
        "        prediction      =\n",
        "        loss_test       =\n",
        "        accuracy_test   =\n",
        "\n",
        "        test_loss_minibatch[k, i]       =\n",
        "        test_accuracy_minibatch[k, i]   ="
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VkVau04wi5rL"
      },
      "source": [
        "## 2] SGD iterations with different weight decay parameter (with mini-batch size = 100) - <font color='red'>EDIT CODE</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIA4Xtghi5rL"
      },
      "outputs": [],
      "source": [
        "# iteration for mini-batch\n",
        "for k in range(num_weight_decay):\n",
        "\n",
        "    size_minibatch  = 100\n",
        "    num_minibatch   =\n",
        "    lmbda           = list_weight_decay[k]\n",
        "\n",
        "    print('mini-batch size = %3d, lmbda = %4.3f' % (size_minibatch, lmbda))\n",
        "\n",
        "    weight =\n",
        "    # initialze seed for generating random number\n",
        "    np.random.seed(0)\n",
        "    # iteration for epoch\n",
        "    for i in tqdm(range(number_epochs)):\n",
        "\n",
        "        index_shuffle   = np.random.permutation(num_data_train)\n",
        "        loss_epoch      = []\n",
        "        accuracy_epoch  = []\n",
        "\n",
        "        for j in range(num_minibatch):\n",
        "\n",
        "            index_minibatch = index_shuffle[j * size_minibatch : (j+1) * size_minibatch]\n",
        "\n",
        "            data    =\n",
        "            label   =\n",
        "\n",
        "            prediction  =\n",
        "            gradient    =\n",
        "            # update network weights:\n",
        "            weight      =\n",
        "\n",
        "            # compute measures after update:\n",
        "            prediction  =\n",
        "            loss        =\n",
        "            accuracy    =\n",
        "\n",
        "            loss_epoch.append(loss)\n",
        "            accuracy_epoch.append(accuracy)\n",
        "\n",
        "        train_loss_mean_weight_decay[k, i] =\n",
        "        train_loss_std_weight_decay[k, i]  =\n",
        "\n",
        "        train_accuracy_mean_weight_decay[k, i] =\n",
        "        train_accuracy_std_weight_decay[k, i]  =\n",
        "\n",
        "        # testing\n",
        "        data    =\n",
        "        label   =\n",
        "\n",
        "        prediction      =\n",
        "        loss_test       =\n",
        "        accuracy_test   =\n",
        "\n",
        "        test_loss_weight_decay[k, i]       =\n",
        "        test_accuracy_weight_decay[k, i]   =\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eQvKviH3rtiM"
      },
      "source": [
        "---\n",
        "# RESULTS\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AJyAHW8brtiE"
      },
      "source": [
        "## 1] plot curve (helper functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95yUiMc4rtiE"
      },
      "outputs": [],
      "source": [
        "def plot_curve(data, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.plot(range(len(data)), data, '-', color='red')\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie2h2ezurtiF"
      },
      "outputs": [],
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.title(title)\n",
        "\n",
        "    lmbda = 0.3\n",
        "\n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', lmbda = lmbda)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2QXqSTArtiF"
      },
      "outputs": [],
      "source": [
        "def plot_curve2(data1, label_data1, data2, label_data2, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.plot(range(len(data1)), data1, '-', color = 'blue', label = label_data1)\n",
        "    plt.plot(range(len(data2)), data2, '-', color = 'red', label = label_data2)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw_o9g9srtiF"
      },
      "outputs": [],
      "source": [
        "def plot_curve_error2(data1_mean, data1_std, data1_label, data2_mean, data2_std, data2_label, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(title)\n",
        "\n",
        "    lmbda = 0.3\n",
        "\n",
        "    plt.plot(range(len(data1_mean)), data1_mean, '-', color = 'blue', label = data1_label)\n",
        "    plt.fill_between(range(len(data1_mean)), data1_mean - data1_std, data1_mean + data1_std, facecolor = 'blue', lmbda = lmbda)\n",
        "\n",
        "    plt.plot(range(len(data2_mean)), data2_mean, '-', color = 'red', label = data2_label)\n",
        "    plt.fill_between(range(len(data2_mean)), data2_mean - data2_std, data2_mean + data2_std, facecolor = 'red', lmbda = lmbda)\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NrV5x4Odi5rN"
      },
      "source": [
        "### 2] Display loss and accuracy curves (over train and test sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBwd30BXM1qk"
      },
      "outputs": [],
      "source": [
        "plot_curve_error2(train_loss_mean_minibatch[0], train_loss_std_minibatch[0], 'mini-batch size = 50',\n",
        "                  train_loss_mean_minibatch[1], train_loss_std_minibatch[1], 'mini-batch size = 100',\n",
        "                  'epoch', 'loss', 'loss (training): for diff. mini-batch sizes')\n",
        "plot_curve_error2(train_accuracy_mean_minibatch[0], train_accuracy_std_minibatch[0], 'mini-batch size = 50',\n",
        "                  train_accuracy_mean_minibatch[1], train_accuracy_std_minibatch[1], 'mini-batch size = 100',\n",
        "                  'epoch', 'accuracy', 'accuracy (training): for diff. mini-batch sizes')\n",
        "plot_curve_error2(train_loss_mean_weight_decay[0], train_loss_std_weight_decay[0], 'weight-decay = 0.001',\n",
        "                  train_loss_mean_weight_decay[1], train_loss_std_weight_decay[1], 'weight-decay = 0.01',\n",
        "                  'epoch', 'loss', 'loss (training): for diff. weight-decay values')\n",
        "plot_curve_error2(train_accuracy_mean_weight_decay[0], train_accuracy_std_weight_decay[0], 'weight-decay = 0.001',\n",
        "                  train_accuracy_mean_weight_decay[1], train_accuracy_std_weight_decay[1], 'weight-decay = 0.01',\n",
        "                  'epoch', 'accuracy', 'accuracy (training): for diff. weight-decay values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM9i5mkCi5rN"
      },
      "outputs": [],
      "source": [
        "plot_curve2(test_loss_minibatch[0], 'mini-batch = 50', test_loss_minibatch[1], 'mini-batch = 100', 'epoch', 'loss', 'loss (testing) for diff. mini-batch sizes')\n",
        "plot_curve2(test_accuracy_minibatch[0], 'mini-batch = 50', test_accuracy_minibatch[1], 'mini-batch = 100', 'epoch', 'accuracy', 'accuracy (testing) for diff. mini-batch sizes')\n",
        "plot_curve2(test_loss_weight_decay[0], 'weight-decay = 0.001', test_loss_weight_decay[1], 'weight-decay = 0.01', 'epoch', 'loss', 'loss (testing) for diff weight-decay values')\n",
        "plot_curve2(test_accuracy_weight_decay[0], 'weight-decay = 0.001', test_accuracy_weight_decay[1], 'weight-decay = 0.01', 'epoch', 'accuracy', 'accuracy (testing) for diff weight-decay values')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOmkO6IQYDn"
      },
      "source": [
        "## 3] Show mispredictions - <font color='red'>EDIT CODE</font>\n",
        "### - Visualize 5 random misclassified train images and 5 random misclassified test images.\n",
        "### - For each, display the image, the predicted distribution, the true and predicted label. Make sure to visualize in a compact and clear manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQMSr5g1T096"
      },
      "outputs": [],
      "source": [
        "... complete here ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HoxoMb9jUECc"
      },
      "source": [
        "### 4] Present final results  - <font color='red'>EDIT CODE</font>\n",
        "### - Display a table with the final results - test accuracies for each of the tested configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va2Q8wd8UcKx"
      },
      "outputs": [],
      "source": [
        "... complete here ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ja0nUf4pUnYY"
      },
      "source": [
        "# 5] Improve results - <font color='red'>EDIT CODE</font>\n",
        "## * Implement two different simple extensions / modifications / configurations that improve the best accuracy achieved above by at least 1%. Do not use a larger number of training epochs.\n",
        "## * Do not make any special effort to get the best result possible, but simply reach this goal.\n",
        "### - Insert the required code blocks here below.\n",
        "### - At the very end (see below), present the final result (test accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JMsnfsTFV3a6"
      },
      "source": [
        "code block 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQKqSTB_VzcG"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfJySJ-V7uL"
      },
      "source": [
        "code block 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syXQHswUUmfL"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5IeVIpHDWAke"
      },
      "source": [
        "code block ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qptS4SZ3WELu"
      },
      "source": [
        "## Final result - <font color='red'>EDIT TEXT and CODE</font>\n",
        "### - Edit this text here to explain very shortly what you have done.\n",
        "### - Use the code block below to present your final results (test accuracy only)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvTMi5AXlIX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
